# Case study: SPEC CPU 2017 benchmarks

As described in the Readme of the project, the full runs of the SPEC CPU 2017 suite require a lot of time. Additionally, due to licensing and copyright restrictions with the proprietary SPEC 2017 suite, we can only provide the used configuration file and detailed instructions how to reproduce our results for people who already bought the SPEC CPU 2017 suite. Please note, that obtaining the raw data for the SPEC benchmarks summarized in Table 4 of the paper takes several CPU weeks.

For this artifact, we therefore do >>**not**<< provide the raw data itself or expect the reviewers to reproduce the full data. In this subdirectory, we instead provide the log outputs of the full SPEC runs with instructions of how to reconstruct Table 4 from these logs.

Furthermore, we provide the two configuration files used during our benchmarks and describe their important elements as well as how they can be utilized to reproduce and regenerate the raw data used in the paper.

![Table4 screenshot](table4.png)

## 1. Spec setup used in our case study

As described in Section 6 of the paper, we performed the SPEC benchmarks multiple times with different settings of the FPU and SSE registers. For this, we modified the configuration file to link our fpu_lib as described in the `lib/` folder. This allowed us to perform the FPU attacks without needing to modify any SPEC source files. For more documentation on this library, see the `lib/` folder of this project or see the detailed description below.

## 2. Log outputs generated by our configuration files

We provide the raw log file outputs as provided on the command line by SPEC when executing the benchmarks. The folder `log_files` contains such output for all of the reported runs.

While these console outputs are technically only secondarily useful, they fully suffice to reproduce Table 4. Since SPEC gives the user an overview at the very end of the run, we can use this overview to understand which benchmarks succeeded and which did not. We provide the simple script `generate_table4.sh` to retrieve these two lines from each log file. In all of our runs, the search string `Producing Raw Reports` was unique in each file and could be used to find the two lines of Succeeded and Errorneous benchmarks (in the lines preceding the search string).

## 3. Reproducing and regenerating the benchmark data

If one wishes to reproduce and regenerate the raw data and run the benchmarks, this section is intended to describe all necessary steps.

The benchmarks were executed with the [SPEC CPU 2017](https://www.spec.org/cpu2017/) benchmark. Once a license has been purchased and the program has been installed, the user needs to adjust the basic configuration file based on the own needs and the utilized system. We performed all benchmarks inside a virtual machine with 16 cores and 16 GB of memory. While the configuration files shared as part of this artifact might not be usable as-is due to custom configurations of the user's machine, this is a list of the essential steps that need to be performed to include the attack in the configuration file. To reproduce and generate the raw data that was used as a basis for the first row in Table 4, the file `attack-fpus.cfg` was used with the following changes:

1. In the section "Compilers" (line 134ff in our attack-fpus.cfg file), the GCC flag `-mfpmath=387` has to be added to the compilation of C, C++, and Fortran files. This requests GCC to not use the SSE but instead to rely only on the x87 FPU.
2. Some lines below, still in the section "Compilers" and in line 142 in our attack-fpus.cfg, the GCC `Libs` flag needs to be extended with the faulty-points library that is part of this artifact in the `lib/` folder. We describe the library further in its subdirectory, but by hooking it in into the SPEC compilation, we can control the behavior of the FPU registers via the environment variables and do not need to change any SPEC benchmarking code. In our configuration file, we simply extended the `LIBS` variable with `LIBS = -Wl,--whole-archive -L/home/ubuntu/faulty-point-unit/lib/ -lfpu_lib -Wl,--no-whole-archive`. Since the library is using a C constructor to perform tasks before execution of the application, using the linker flags `--whole-archive` enforces the inclusion of the whole library even if the linker assumes parts of it are not used (which they are based on environment variables).

Besides these two modifications to the compilation and linking process, we only set up the configuration files for the used system, following the standard installation steps of SPEC CPU 2017 (64 bit operating system, 16 cores, etc).

To run this configuration file and generate the benchmark data reported in the paper, we performed multiple invocations of:

```bash
export FPU_ROUND=DOWN
export FPU_PRECISION=SINGLE
export FPU_VERBOSE=FALSE
runcpu --config=attack-fpus --size=ref --copies=1 --noreportable --iterations=1 --threads=4 intrate intspeed fprate fpspeed
```

Each invocation used a different set of FPU_ROUND variables as described in the usage of the faulty_points library. These runs then generated the raw data for the first column in Table 4: Single precision.

To generate the data for the second row in Table 4, we used the configuration file called `attack-fpus-baseline.cfg` with exactly the same steps as above but omitting Step 1 (setting of `-mfpmath=387` to enforce the usage of the x87 FPU). Since GCC defaults to the SSE when this flag is not set, there are otherwise no differences in the configuration files between the two rows in the Table.
